# -*- coding: utf-8 -*-
"""Khromov Daniil.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1TaVkN1isaZdO8ZD75eDDnV8j3CQoZRwz

## Imports
"""

!pip install catboost

import pandas as pd
from catboost import CatBoostRegressor, Pool
import matplotlib.pyplot as plt
from sklearn.model_selection import train_test_split
import numpy as np
import seaborn as sns
from statsmodels.tsa.stattools import adfuller
from statsmodels.tsa.arima.model import ARIMA
plt.rcParams.update({'figure.figsize':(9,7), 'figure.dpi':120})

"""## Mount GDrive"""

# Commented out IPython magic to ensure Python compatibility.
from google.colab import drive
drive.mount('/content/drive/')
# %cd /content/drive/MyDrive/NewsStocksData
!ls

"""## Load data & transformations

The dataset includes the following columns:

**Date**: The date on which the stock market data was recorded.

**Open**: The opening price of the asset on the given date.

**High**: The highest price of the asset on the given date.

**Low**: The lowest price of the asset on the given date.

**Close**: The closing price of the asset on the given date. Note that this price does not take into account any after-hours trading that may have occurred after the market officially closed.

**Adj Close**: The adjusted closing price of the asset on the given date. This price takes into account any dividends, stock splits, or other corporate actions that may have occurred, which can affect the stock price.

**Volume**: The total number of shares of the asset that were traded on the given date.
"""

df = pd.read_excel('yahoo_data.xlsx')

df.to_csv('data.csv')

df.head()

# delete close as we have adjusted close which is more accurate
df = df.drop('Close*', axis=1)

# string to datetime and sort ascending
df['Date'] = pd.to_datetime(df['Date'])
df = df.sort_values('Date')
df.to_csv('data.csv') # save

df.head()

"""## EDA"""

plt.plot(df['Date'], df['Adj Close**'])
plt.title('Adjusted Close Price')
plt.xlabel('Date')
plt.ylabel('Price ($)')
plt.show()

correlation_matrix = df.corr()
plt.figure(figsize=(6, 4))
sns.heatmap(correlation_matrix, annot=True, cmap='coolwarm')
plt.title("Correlation Matrix")
plt.show();

"""**Volume has very low correlation with other features!**

Checking NAN values:
"""

df.isna().sum()

"""Volume traded"""

plt.scatter(df['Date'], df['Volume'])
plt.title('Volume traded')
plt.xlabel('Date')
plt.ylabel('Volume ($)')
plt.show()

"""Resampling Adj Close column month wise"""

df2 = df.set_index('Date')
df2['Adj Close**'].resample('M').mean().plot()
plt.title('Adjusted Close price by month')
plt.xlabel('Date')
plt.ylabel('Price ($)')
plt.show()

"""Resampling Adj Close column year wise"""

df2['Adj Close**'].resample('Y').mean().plot()
plt.title('Adjusted Close price by year')
plt.xlabel('Date')
plt.ylabel('Price ($)')
plt.show()

"""## Dickey-Fuller Test for stationary"""

def check_mean_std(ts):
    #Rolling statistics
    rolmean = ts.rolling(6).mean()
    rolstd = ts.rolling(6).std()
    plt.figure(figsize=(22,10))
    orig = plt.plot(ts, color='red',label='Original')
    mean = plt.plot(rolmean, color='black', label='Rolling Mean')
    std = plt.plot(rolstd, color='green', label = 'Rolling Std')
    plt.xlabel("Date")
    plt.ylabel("Mean Closing Price")
    plt.title('Rolling Mean & Standard Deviation')
    plt.legend()
    plt.show()

def check_adfuller(df):
  result=adfuller(df)
  print('Test Statistic: %f' %result[0])
  print('p-value: %f' %result[1])
  print('Critical values:')
  for key, value in result[4].items ():
     print('\t%s: %.3f' %(key, value))

"""# ARIMA

## Finding parameters for ARIMA algorithms

These parameters can be explained as follows

1.   p is the number of autoregressive terms,
2.   d is the number of nonseasonal differences,
3.   q is the number of lagged forecast errors in the prediction equation.
"""

data = df[['Adj Close**']]
data = data.set_index(df['Date'])
data.head(3)

data.plot()

check_mean_std(data)
check_adfuller(data)

"""Our data is not stationary => we need to difference it!"""

data = data.diff().dropna()

check_mean_std(data)
check_adfuller(data)

"""now it is stationary so we can continue

### Finding d parameter

1 Differencing has less noise so we try it => d = 1

### Finding p parameter
"""

from statsmodels.graphics.tsaplots import plot_pacf, plot_acf
plot_pacf(data.dropna())
plt.show()

"""We can set p = 7

### Finding q value
"""

plot_acf(data.dropna(), lags=12)
plt.show()

"""q value can be 7

# ARIMAX
"""

TEST_SIZE = int(len(df)*0.25)

X = df[['Adj Close**']]
X = X.set_index(df['Date'])
X = X.dropna()
train, test = X[0:-TEST_SIZE], X[-TEST_SIZE:]
exogenous_train, exogenous_test = df['Open'][0:-TEST_SIZE].values, df['Open'][-TEST_SIZE:].values

model_arima = ARIMA(endog = train.values, exog=exogenous_train, order=(7, 1, 2)).fit()

forecast = model_arima.forecast(TEST_SIZE, exog=exogenous_test)
forecast = pd.DataFrame(forecast)
forecast = forecast.set_index(test.index)

get_ipython().run_line_magic('matplotlib', 'inline')
plt.rcParams["figure.figsize"] = [15, 7]
plt.plot(test, label='Test')
plt.plot(forecast, label='Predicted')
leg = plt.legend(loc='upper center')
plt.show()

from sklearn.metrics import mean_squared_error

RMSE = (mean_squared_error(test, forecast))**0.5

def mape(y_true, y_pred):
    return np.mean(np.abs((y_pred - y_true) / y_true)) * 100

print('RMSE', RMSE)
print('The Mean Absolute Percentage Error is: %.3f' % mape(np.array(test), forecast),'%.')

"""# Catboost"""

model = CatBoostRegressor(loss_function='RMSE', silent=True)

df = df.drop('Adj Close Stationary', axis=1)
X = df.drop('Adj Close**', axis=1)
y = df['Adj Close**']
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, shuffle=False)

model = CatBoostRegressor(iterations=200,
                          learning_rate=0.1,
                          depth=4)
model.fit(X_train, y_train) # Fit model
preds = model.predict(X_test) # Get predictions

plt.plot(df['Date'], df['Adj Close**'])
plt.plot(X_test['Date'], preds)
plt.show()

model.best_score_